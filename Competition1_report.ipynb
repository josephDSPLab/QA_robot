{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a QA-Robot by Neural Network\n",
    "In this part, we trained a neural network model for the **QA-robot** in competition 1. This part will be organized in three parts: (a) Word embedding model, (b) generating training data and computing the features, and (c) the graph and the session of our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word enbedding\n",
    "To process the natural language sentence, we have to encode each chinese word before training the neural network model. We choose FastText as the word embedding model. All the sentences in training data were partitioned into word level by jieba, and converted into the format which can be read by gensim fasttext model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Processing training data as the same in the TA's example code and save in *.npy file format\n",
    "# Read in the processed and partitioned word file and convert it into the format of gensim\n",
    "import numpy as np\n",
    "cut_programs = np.load('cut_Programs.npy')\n",
    "cut_Question = np.load('cut_Questions.npy')\n",
    "print(sum([len(p) for p in cut_programs]))\n",
    "\n",
    "with open('split_word_Programs.txt', 'w', encoding='utf-8') as output:\n",
    "    for i, program in enumerate(programs):\n",
    "        episodes = len(program)\n",
    "        print('Processing the %d programs' % i)\n",
    "        for episode in range(episodes):\n",
    "            for line in cut_programs[i][episode]:\n",
    "                line_space = \" \".join(line)\n",
    "                output.write(line_space)\n",
    "                output.write('\\n')\n",
    "\n",
    "with open('split_word_Questions.txt', 'w', encoding='utf-8') as output:\n",
    "    n = len(questions)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(6):\n",
    "            for line in cut_Question[i][j]:\n",
    "                line_space = \" \".join(line)\n",
    "                output.write(line_space)\n",
    "                output.write('\\n')\n",
    "                \n",
    "# Train a fasttext model\n",
    "from gensim.models import word2vec, doc2vec, FastText\n",
    "import numpy as np\n",
    "\n",
    "vec_size = 128\n",
    "sentences = word2vec.LineSentence('split_word_Programs.txt')\n",
    "model = FastText(sentences, size=vec_size, window=5,min_count=1)\n",
    "model.save(\"fasttext\"+str(vec_size)+\".model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating training data\n",
    "To train a QA-Robot model by the corpus from the normal programs, the data should be transformed into 1 question and 6 answer candidates format to. We define two consecutive sentences are a pair of correct question and answer. With exclusively and randomly selected 5 sentences, a complete QA pair is formed. Due to the tediousness of the lengthy codes, the codes are not shown in this report. Please refer to `gen_data.py` and `gen_data_infer.py` for the details of implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the features\n",
    "The most important problem of the feature extraction is how to deal with the variable length sentence. Therefore, we have tried 3 types of feature to unify the word vectors of different sentences:\n",
    "1. Padding the word vectors to the length which is equals to the maximun legnth of the sentence in the training data. The data are padded with **wrapping values** instead of **zeros** for avoiding the numerical imbalance. The dimension of this feature is [*MAX_LEN*, *VECTOR_SIZE*].\n",
    "2. Averaging all the word vectors (along the axis of words). The dimension of this feature is [*VECTOR_SIZE*].\n",
    "3. Calculating the standard deviation from all the word vectors of the sentence. The dimension of this feature is also [*VECTOR_SIZE*].\n",
    "4. Calculating the Word Mover's Distance (WMD) between the question and the answer candidate. The dimension of this feature is [*1*].\n",
    "This codes are part of `gen_data.py` and `gen_data_infer.py`. Note that `self.w2vembedd2` is a FastText model and `self.w2vembedd` is a Doc2Vec model which was not used in the final version.\n",
    "\n",
    "Several combinations of the features and corresponding were tested, more details and results will be discuessed in the next parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence2vec(self, sentence_idx):\n",
    "    sentvec = sent = 0\n",
    "    word_c2 = word_c = 0\n",
    "    sent_sq = 0\n",
    "    wv_seq = np.empty([0,VECTOR_SIZE])\n",
    "    for i, w in enumerate(self.sentences[sentence_idx]):\n",
    "        if MAX_LEN <= word_c:\n",
    "\n",
    "            break\n",
    "        if w.isalnum():\n",
    "            try:\n",
    "                wv_seq = np.vstack([wv_seq, self.w2vembedd2.wv[[w]]])\n",
    "                word_c += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "            try:\n",
    "                sentvec += self.w2vembedd.wv[w]\n",
    "                #sent_var += self.w2vembedd2.wv[w]**2\n",
    "                word_c2 += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "    if word_c > 0:\n",
    "        sent = np.mean(wv_seq,0)\n",
    "        sent_var = np.std(wv_seq,0)\n",
    "    if word_c2 > 0:\n",
    "        sentvec = sentvec / word_c2\n",
    "    return np.concatenate([sentvec,sent,sent_var], 0), wv_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The graph and the session of our neural network\n",
    "We assume that different types of feature preserve its characteristic. The following figure is the overal scheme of our model:\n",
    "\n",
    "|  ![alt text](fig1.png \"Title\")  | ![alt text](fig2.png \"Title\")  |\n",
    "|--------|--------|\n",
    "\n",
    "![alt text](fig3.png \"Title\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def question(q_sentence, training_flag):\n",
    "    l1_regularizer = tf.contrib.layers.l1_regularizer(\n",
    "        scale=1e-5, scope=None)\n",
    "    frame_out = tf.layers.conv2d(q_sentence, 256, [1,1], kernel_initializer=tf.contrib.layers.xavier_initializer(), activation=tf.nn.relu)\n",
    "    frame_att = tf.layers.conv2d(q_sentence, 256, [1,1], kernel_initializer=tf.contrib.layers.xavier_initializer(\n",
    "        ), activation=tf.nn.sigmoid, kernel_regularizer=l1_regularizer)\n",
    "    frame_out *= frame_att\n",
    "    pool_size = frame_out.get_shape().as_list()[2]\n",
    "    frame_out = tf.layers.average_pooling2d(\n",
    "        frame_out, pool_size=[1,pool_size], strides=1)\n",
    "    return frame_out\n",
    "\n",
    "\n",
    "def answer(a_sentence, training_flag):\n",
    "    l1_regularizer = tf.contrib.layers.l1_regularizer(\n",
    "        scale=1e-5, scope=None)\n",
    "    frame_out = tf.layers.conv2d(a_sentence, 256, [1,1], kernel_initializer=tf.contrib.layers.xavier_initializer(), activation=tf.nn.relu)\n",
    "    frame_att = tf.layers.conv2d(a_sentence, 256, [1,1], kernel_initializer=tf.contrib.layers.xavier_initializer(\n",
    "        ), activation=tf.nn.sigmoid, kernel_regularizer=l1_regularizer)\n",
    "    frame_out *= frame_att\n",
    "    pool_size = frame_out.get_shape().as_list()[2]\n",
    "    frame_out = tf.layers.average_pooling2d(\n",
    "        frame_out, pool_size=[1,pool_size], strides=1)\n",
    "    return frame_out\n",
    "\n",
    "\n",
    "def similar(logits_q, logits_a, training_flag, p,is_batchnorm=False,name='similarity'):\n",
    "\n",
    "    qa_pair = tf.concat([logits_q * logits_a,logits_q**2,logits_a**2], -1)\n",
    "    qa_pair = denselayer(qa_pair, 256, tf.nn.relu, is_batchnorm=is_batchnorm,\n",
    "                         is_dropout=True, prob=p, is_training=training_flag, repeat=1)\n",
    "    fc_out = denselayer(qa_pair, 64, tf.nn.relu, is_batchnorm=is_batchnorm,\n",
    "                        is_dropout=False, prob=1, is_training=training_flag, repeat=1)\n",
    "    return fc_out\n",
    "\n",
    "def selection(simi_all, training_flag, p):\n",
    "    qa_pair = simi_all\n",
    "    qa_pair = denselayer(qa_pair, 64, tf.nn.selu, is_batchnorm=False,\n",
    "                         is_dropout=False, prob=1, is_training=training_flag, repeat=1)\n",
    "    fc_out = denselayer(qa_pair, 6, None, is_batchnorm=False,\n",
    "                        is_dropout=False, prob=1, is_training=training_flag, repeat=1)\n",
    "    return fc_out\n",
    "\n",
    "dim = 1024\n",
    "def dim_reduce(x_input, name, window=1,dim = 1024):\n",
    "    with tf.variable_scope(name):\n",
    "        l1_regularizer = tf.contrib.layers.l1_regularizer(\n",
    "            scale=1e-5, scope=None)\n",
    "        x = tf.layers.conv1d(x_input, dim, [\n",
    "                             window], kernel_initializer=tf.contrib.layers.xavier_initializer(), activation=tf.nn.selu)\n",
    "        \n",
    "        xa = tf.layers.conv1d(x, dim, [\n",
    "                             1], kernel_initializer=tf.contrib.layers.xavier_initializer(), activation=tf.nn.selu)\n",
    "        x_att = tf.layers.conv1d(x, dim, [1], kernel_initializer=tf.contrib.layers.xavier_initializer(\n",
    "        ), activation=tf.nn.sigmoid, kernel_regularizer=l1_regularizer)\n",
    "        x = xa * x_att\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "    model = models.Word2Vec.load('doc2vec128.model')\n",
    "    model2 = models.FastText.load('fasttext128.model')\n",
    "    #model.init_sims(replace=True)\n",
    "    model2.init_sims(replace=True)\n",
    "    train_dataset = gen_dataset.dataset(w2v=model, w2v2=model2, val_ratio=0.2)\n",
    "    test_dataset = gen_dataset_infer.dataset(\n",
    "        w2v=model, w2v2=model2, val_ratio=0)\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x_input = tf.placeholder(\n",
    "            tf.float32, [None, 7, gen_dataset.VECTOR_SIZE * 3])\n",
    "        x_seq = tf.placeholder(\n",
    "            tf.float32, [None, 7, gen_dataset.MAX_LEN, gen_dataset.VECTOR_SIZE])\n",
    "        y_label = tf.placeholder(tf.float32, [None, 6])\n",
    "        sim_tensor = tf.placeholder(tf.float32, [None, 6])\n",
    "        training_flag = tf.placeholder(tf.bool)\n",
    "        prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        #module for the all WVs\n",
    "        simi_mat_red = tf.reshape(question(x_seq[:,0:1,:,:], training_flag) * answer(x_seq[:,1:,:,:], training_flag),[-1,256])\n",
    "        \n",
    "        #module for the avg. of WVs\n",
    "        x1q = dim_reduce(\n",
    "            x_input[:, 0:1, gen_dataset.VECTOR_SIZE:gen_dataset.VECTOR_SIZE * 2], 'red_1_q')\n",
    "        x1a = dim_reduce(\n",
    "            x_input[:, 1:, gen_dataset.VECTOR_SIZE:gen_dataset.VECTOR_SIZE * 2], 'red_1_a')\n",
    "        \n",
    "        #module for the std. of WVs\n",
    "        x3q = dim_reduce(\n",
    "            x_input[:, 0:1, gen_dataset.VECTOR_SIZE*2:gen_dataset.VECTOR_SIZE * 3], 'red_3_q')\n",
    "        x3a = dim_reduce(\n",
    "            x_input[:, 1:, gen_dataset.VECTOR_SIZE*2:gen_dataset.VECTOR_SIZE * 3], 'red_s3_a')\n",
    "\n",
    "        x = tf.concat([x1q,x1a], 1)\n",
    "        x3 = tf.concat([x3q,x3a], 1)\n",
    "\n",
    "        x_question = tf.reshape(tf.tile(x[:, 0:1, :],[1,6,1]),[-1,x.shape[-1]])\n",
    "        x_ans = tf.reshape(x[:, 1:, :],[-1,x.shape[-1]])\n",
    "        x3_question = tf.reshape(tf.tile(x3[:, 0:1, :],[1,6,1]),[-1,x.shape[-1]])\n",
    "        x3_ans = tf.reshape(x3[:, 1:, :],[-1,x.shape[-1]])\n",
    "\n",
    "        a = similar(x_question, x_ans, training_flag, prob)\n",
    "        b = similar(x3_question, x3_ans, training_flag, prob,is_batchnorm=True)\n",
    "\n",
    "        sim_exp = tf.reshape(sim_tensor, [-1,1])\n",
    "        simi_all = tf.concat([simi_mat_red,sim_exp], -1)\n",
    "        simi_single = tf.reshape(simi_all, [-1, simi_all.shape[-1]])\n",
    "        single_out = denselayer(simi_single, 64, tf.nn.selu, is_batchnorm=False,\n",
    "                         is_dropout=False, prob=1, is_training=training_flag, repeat=1)\n",
    "        single_out = denselayer(single_out, 1, None, is_batchnorm=False,\n",
    "                         is_dropout=False, prob=1, is_training=training_flag, repeat=1)\n",
    "        simi_all = tf.reshape(simi_all, [-1, simi_all.shape[-1] * 6])\n",
    "\n",
    "        out = selection(simi_all, training_flag, prob)\n",
    "        sigmoid_out = tf.nn.sigmoid(out)\n",
    "        pred = tf.argmax(softmax_out,1)\n",
    "        reg_ws = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss_single = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=single_out,  labels=tf.reshape(y_label,[-1,1])))\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=out,  labels=y_label))\n",
    "        loss = tf.reduce_sum(reg_ws) + loss_single + loss\n",
    "\n",
    "        acc = tf.equal(tf.argmax(y_label, 1), pred)\n",
    "        acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        writer = tf.summary.FileWriter(graph_dir, tf.get_default_graph())\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    best_acc = 0\n",
    "    early_count = 0\n",
    "    ans = np.load('label.npy')\n",
    "    with tf.Session(config=config, graph=g) as sess:\n",
    "        writer = tf.summary.FileWriter(graph_dir, graph=sess.graph)\n",
    "        sess.run(init)\n",
    "        for i in range(3000):\n",
    "            s, label, sim, wv_seq, end_flag = train_dataset.getbatch(\n",
    "                mode='train', batch_size=batch_size)\n",
    "            if end_flag != gen_dataset.IS_EPOCH_END:\n",
    "                lo, _, a = sess.run([loss, optimizer, acc], feed_dict={\n",
    "                                    x_input: s, y_label: label, sim_tensor: sim, x_seq:wv_seq, training_flag: True, prob: 0.8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "We found that too small batch size leads the training instability (size < 128). Therefore, we tried 512, 1024, 2048 as the batch size. For selecting input feature, we first several model by only one type of feature. Surprisingly, the average and stadard deviation of the word vectors give a better results. We further try the different combination of the feature to train different model.\n",
    "\n",
    "In the end of the competition, we selected 2 results for the deep learning methods. One is from the best parameters, another is from voting by different training results under different parameters.\n",
    "\n",
    "| |  Best  | Voting  |\n",
    "|----|--------|--------|\n",
    "|Public score|0.612|0.648|\n",
    "|Private score|0.688|0.692|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
